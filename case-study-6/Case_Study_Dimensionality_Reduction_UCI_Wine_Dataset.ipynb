{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc41cfb",
   "metadata": {
    "id": "4dc41cfb"
   },
   "source": [
    "## In-Class Activity: Dimensionality Reduction on the UCI Wine Dataset\n",
    "\n",
    "Goal: see how **PCA + scaling** impacts (1) a simple classifier and (2) **2D visual separability**, then compare PCA to other popular 2D embedding methods.\n",
    "\n",
    "Dataset: **Wine** (UCI / scikit-learn) — 178 samples, 13 continuous features, 3 classes.\n",
    "\n",
    "Workflow:\n",
    "- Load the dataset and note that features have **different units / ranges**\n",
    "- Train/test split (70% / 30%)\n",
    "- **Part I (PCA + Logistic Regression):** evaluate how PCA behaves with different scalers and how that affects a linear classifier\n",
    "- **Part II (Other embeddings, plots only):** generate 2D plots for Kernel PCA, LDA, and UMAP (no classifier)\n",
    "\n",
    "Part I — PCA + Logistic Regression (full 13D → 2D):\n",
    "- Run **PCA(2) without scaling** → train + evaluate Logistic Regression in PCA space\n",
    "- Run **Min–Max → PCA(2)** → train + evaluate Logistic Regression in PCA space\n",
    "- Run **Standard → PCA(2)** → train + evaluate Logistic Regression in PCA space\n",
    "- Run **Robust → PCA(2)** → train + evaluate Logistic Regression in PCA space\n",
    "- For each case: report train/test performance using classification report + confusion matrix\n",
    "- Also plot the 2D PCA space to visually compare separability across scalers\n",
    "\n",
    "Part II — Other 2D embeddings (plots only, no classifier):\n",
    "- **Kernel PCA (RBF)** on **standard-scaled** data → 2D scatter plot\n",
    "- **LDA** on **standard-scaled** data → 2D scatter plot\n",
    "- **UMAP** on **standard-scaled** data → 2D scatter plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3bc0e",
   "metadata": {
    "id": "eeb3bc0e"
   },
   "source": [
    "<details>\n",
    "<summary><b>Workflow (high level)</b></summary>\n",
    "\n",
    "1. Load the Wine dataset and inspect columns/features  \n",
    "2. Train/test split (70% / 30%, stratified)  \n",
    "3. Part I: fit the three Logistic Regression pipelines and compare results  \n",
    "4. Visualization: recreate PCA plots (no scaling vs scaling) with decision regions  \n",
    "5. Part II: fit Kernel PCA / LDA / UMAP on standardized data and plot 2D embeddings  \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93360ed1",
   "metadata": {
    "id": "93360ed1"
   },
   "outputs": [],
   "source": "# ============================================================\n# Imports + settings\n# ============================================================\nimport os\nos.environ[\"NUMBA_OPT\"] = \"0\"  # work around LLVM JIT crash on ARM64\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)"
  },
  {
   "cell_type": "markdown",
   "id": "8012a8f7",
   "metadata": {
    "id": "8012a8f7"
   },
   "source": [
    "<details>\n",
    "<summary><b>Load the dataset</b></summary>\n",
    "\n",
    "We load the Wine dataset and put it into a pandas DataFrame so it is easier to work with.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93479a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "a93479a7",
    "outputId": "3a0d1712-ce79-4637-be2e-3763e63daf35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (178, 14)\n",
      "Feature columns: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Target name: ['class_0' 'class_1' 'class_2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load Wine (features + labels)\n",
    "# ============================================================\n",
    "data = load_wine(as_frame=True)\n",
    "df = data.frame.copy()  # includes 'target'\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Feature columns:\", data.feature_names)\n",
    "print(\"Target name:\", data.target_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31820682",
   "metadata": {
    "id": "31820682"
   },
   "source": [
    "<details>\n",
    "<summary><b>Train/test split (70/30)</b></summary>\n",
    "\n",
    "We do a stratified split so each class keeps roughly the same proportion in train and test.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff6b51f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dff6b51f",
    "outputId": "21ebd630-0495-411e-b45a-2a2473e8c0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (124, 13) Test shape: (54, 13)\n",
      "Train label counts: [41 50 33]\n",
      "Test  label counts: [18 21 15]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Train/test split (70% / 30%), stratified\n",
    "# ============================================================\n",
    "X = df.drop(columns=[\"target\"]).values\n",
    "y = df[\"target\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train label counts:\", np.bincount(y_train))\n",
    "print(\"Test  label counts:\", np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde6082",
   "metadata": {
    "id": "edde6082"
   },
   "source": [
    "<details>\n",
    "<summary><b>Helper functions (evaluation + plotting)</b></summary>\n",
    "\n",
    "- `evaluate_model(...)` prints accuracy + classification report + confusion matrix  \n",
    "- `plot_decision_regions_2d(...)` draws decision regions for a 2D model  \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c8183dc",
   "metadata": {
    "id": "9c8183dc"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def evaluate_model(name, model, X_tr, y_tr, X_te, y_te):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    yhat_tr = model.predict(X_tr)\n",
    "    yhat_te = model.predict(X_te)\n",
    "\n",
    "    tr_acc = accuracy_score(y_tr, yhat_tr)\n",
    "    te_acc = accuracy_score(y_te, yhat_te)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 78)\n",
    "    print(f\"{name}\")\n",
    "    print(\"-\" * 78)\n",
    "    print(f\"Train accuracy: {tr_acc:.4f}\")\n",
    "    print(f\"Test  accuracy: {te_acc:.4f}\")\n",
    "\n",
    "    print(\"\\n[TEST] Classification report:\")\n",
    "    print(classification_report(y_te, yhat_te, digits=4, zero_division=0))\n",
    "    print(\"[TEST] Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(y_te, yhat_te))\n",
    "\n",
    "    return {\"name\": name, \"train_acc\": tr_acc, \"test_acc\": te_acc}\n",
    "\n",
    "def plot_decision_regions_2d(ax, X2, y, clf, title, step=0.02):\n",
    "    # Fit on the provided 2D data\n",
    "    clf.fit(X2, y)\n",
    "\n",
    "    x1_min, x1_max = X2[:, 0].min() - 1.0, X2[:, 0].max() + 1.0\n",
    "    x2_min, x2_max = X2[:, 1].min() - 1.0, X2[:, 1].max() + 1.0\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(x1_min, x1_max, step),\n",
    "        np.arange(x2_min, x2_max, step)\n",
    "    )\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    zz = clf.predict(grid).reshape(xx1.shape)\n",
    "\n",
    "    ax.contourf(xx1, xx2, zz, alpha=0.15)\n",
    "    markers = [\"^\", \"s\", \"o\"]\n",
    "    for c in np.unique(y):\n",
    "        ax.scatter(X2[y == c, 0], X2[y == c, 1], marker=markers[int(c)], s=35, label=f\"class {int(c)+1}\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"1st component\")\n",
    "    ax.set_ylabel(\"2nd component\")\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98132c6",
   "metadata": {
    "id": "d98132c6"
   },
   "source": [
    "# Part I — Logistic Regression comparisons\n",
    "\n",
    "We keep the classifier the same (Logistic Regression) and only change **scaling** and **PCA**.\n",
    "\n",
    "Notes:\n",
    "- PCA is fit on the **training set** only.\n",
    "- The classifier is trained on either the original 13D space or the reduced 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16217fa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "16217fa1",
    "outputId": "50f66742-0804-4d2d-d9b2-805afdebe6a2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n# Part I: 3 Logistic Regression pipelines\n# ============================================================\nlr = LogisticRegression(max_iter=5000)\n\npipe_lr_unscaled = Pipeline(steps=[\n    (\"clf\", lr),\n])\n\npipe_pca_unscaled = Pipeline(steps=[\n    (\"pca\", PCA(n_components=2, random_state=RANDOM_STATE)),\n    (\"clf\", lr),\n])\n\npipe_scaled_pca = Pipeline(steps=[\n    (\"scale\", StandardScaler()),\n    (\"pca\", PCA(n_components=2, random_state=RANDOM_STATE)),\n    (\"clf\", lr),\n])\n\nresults = []\nresults.append(evaluate_model(\"1) Logistic Regression (unscaled, 13D)\", pipe_lr_unscaled, X_train, y_train, X_test, y_test))\nresults.append(evaluate_model(\"2) PCA(2) on unscaled data -> Logistic Regression\", pipe_pca_unscaled, X_train, y_train, X_test, y_test))\nresults.append(evaluate_model(\"3) StandardScaler -> PCA(2) -> Logistic Regression\", pipe_scaled_pca, X_train, y_train, X_test, y_test))\n\npd.DataFrame(results)[[\"name\",\"train_acc\",\"test_acc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a20b6b",
   "metadata": {
    "id": "d9a20b6b"
   },
   "source": [
    "# PCA visualization — 2D PCA under different scaling choices\n",
    "\n",
    "We run PCA using the **full Wine feature vector (13 features)** and visualize the **2D PCA embedding** under four preprocessing choices:\n",
    "\n",
    "1. **No scaling**\n",
    "2. **Min–Max scaling**\n",
    "3. **Standard scaling**\n",
    "4. **Robust scaling**\n",
    "\n",
    "We also overlay **logistic regression decision regions** in each 2D PCA space (for visualization only).\n",
    "To avoid Colab RAM crashes, we use a **coarser grid** for decision regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec491f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "fec491f1",
    "outputId": "b6e5e3a5-1836-4882-bb6b-8ea83816c236"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n# Memory-safe PCA visualization (13D -> 2D) under scaling choices\n# ============================================================\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\n\ndef plot_decision_regions_2d_light(ax, X2, y, clf, title, grid_n=180):\n    \"\"\"\n    Lighter version of decision-region plotting:\n    - coarse grid (grid_n x grid_n)\n    - float32 to reduce memory\n    \"\"\"\n    X2 = X2.astype(np.float32, copy=False)\n    y  = y.astype(np.int32, copy=False)\n\n    # Fit classifier in the 2D space\n    clf.fit(X2, y)\n\n    # Create a coarse mesh\n    pad = 0.7\n    x_min, x_max = X2[:, 0].min() - pad, X2[:, 0].max() + pad\n    y_min, y_max = X2[:, 1].min() - pad, X2[:, 1].max() + pad\n\n    xx = np.linspace(x_min, x_max, grid_n, dtype=np.float32)\n    yy = np.linspace(y_min, y_max, grid_n, dtype=np.float32)\n    XX, YY = np.meshgrid(xx, yy)\n\n    grid = np.column_stack([XX.ravel(), YY.ravel()]).astype(np.float32, copy=False)\n    Z = clf.predict(grid).reshape(XX.shape)\n\n    # Decision regions\n    ax.contourf(XX, YY, Z, alpha=0.18)\n\n    # Scatter points\n    markers = [\"^\", \"s\", \"o\", \"x\", \"D\", \"P\"]\n    for c in np.unique(y):\n        ax.scatter(\n            X2[y == c, 0], X2[y == c, 1],\n            marker=markers[int(c) % len(markers)],\n            s=28,\n            label=f\"class {int(c)+1}\"\n        )\n\n    ax.set_title(title)\n    ax.set_xlabel(\"PC1\")\n    ax.set_ylabel(\"PC2\")\n    ax.legend(loc=\"best\")\n\n# ----------------------------\n# Data\n# ----------------------------\nX_full = X.copy()\ny_full = y.copy()\n\n# Scaling options\nscalers = {\n    \"No scaling\": None,\n    \"Min–Max scaled\": MinMaxScaler(),\n    \"Standard scaled\": StandardScaler(),\n    \"Robust scaled\": RobustScaler(),\n}\n\n# Classifier (visualization only)\nlr_vis = LogisticRegression(max_iter=3000)\n\n# Plot\nfig, axes = plt.subplots(1, 4, figsize=(20, 4))\n\nfor ax, (label, scaler) in zip(axes, scalers.items()):\n    # Scale (or not)\n    if scaler is None:\n        X_proc = X_full.astype(np.float32, copy=False)\n    else:\n        X_proc = scaler.fit_transform(X_full).astype(np.float32, copy=False)\n\n    # PCA to 2D\n    X_pca = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(X_proc).astype(np.float32, copy=False)\n\n    # Plot (coarse grid to avoid RAM spikes)\n    plot_decision_regions_2d_light(ax, X_pca, y_full, lr_vis, f\"PCA (2D): {label}\", grid_n=160)\n\nplt.tight_layout()\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132003a",
   "metadata": {
    "id": "c132003a"
   },
   "source": [
    "# Part II — Kernel PCA, LDA, and UMAP (plots only)\n",
    "\n",
    "For Part II we standardize **all 13 features**, then create **2D embeddings**.\n",
    "\n",
    "Notes:\n",
    "- **Kernel PCA (RBF)** is a nonlinear extension of PCA.\n",
    "- **LDA** is supervised (uses labels) and tries to maximize class separability.\n",
    "- **UMAP** is a nonlinear manifold method (often good for visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de0628",
   "metadata": {
    "id": "d9de0628"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install UMAP (Colab)\n",
    "# ============================================================\n",
    "!pip -q install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb73cb",
   "metadata": {
    "id": "b8eb73cb"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Standardize features (fit on train, transform train/test)\n",
    "# ============================================================\n",
    "scaler = StandardScaler()\n",
    "Xtr_s = scaler.fit_transform(X_train)\n",
    "Xte_s = scaler.transform(X_test)\n",
    "\n",
    "def plot_embedding_2d(Z, y, title):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    markers = [\"^\", \"s\", \"o\"]\n",
    "    for c in np.unique(y):\n",
    "        plt.scatter(Z[y == c, 0], Z[y == c, 1], marker=markers[int(c)], s=35, label=f\"class {int(c)+1}\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"1st component\")\n",
    "    plt.ylabel(\"2nd component\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FNkfYEispWpT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "FNkfYEispWpT",
    "outputId": "82b1541e-87c3-446d-bb23-5bef7e897cbd"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Q4: Kernel PCA (RBF) — effect of gamma\n",
    "# ============================================================\n",
    "gammas = [0.1, 0.5, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "fig.suptitle(\"Kernel PCA (RBF) — effect of gamma (test set)\", fontsize=13)\n",
    "\n",
    "for ax, g in zip(axes, gammas):\n",
    "    kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=g, fit_inverse_transform=False)\n",
    "    Z_tr = kpca.fit_transform(Xtr_s)\n",
    "    Z_te = kpca.transform(Xte_s)\n",
    "\n",
    "    markers = [\"^\", \"s\", \"o\"]\n",
    "    for c in np.unique(y_test):\n",
    "        ax.scatter(Z_te[y_test == c, 0], Z_te[y_test == c, 1],\n",
    "                   marker=markers[int(c)], s=40, label=f\"class {int(c)+1}\")\n",
    "    ax.set_title(f\"gamma = {g}\")\n",
    "    ax.set_xlabel(\"1st component\")\n",
    "    ax.set_ylabel(\"2nd component\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w409fML-YgDv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "w409fML-YgDv",
    "outputId": "08b49455-6904-4e39-b3ff-a2af60a4cd83"
   },
   "outputs": [],
   "source": [
    "# 2) LDA (supervised)\n",
    "lda = LDA(n_components=2)\n",
    "Z_lda = lda.fit_transform(Xtr_s, y_train)\n",
    "Z_lda_te = lda.transform(Xte_s)\n",
    "plot_embedding_2d(Z_lda_te, y_test, \"LDA on standardized data (test set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3GjJsHX2Yjmd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "3GjJsHX2Yjmd",
    "outputId": "74957a01-dcaa-42da-f9a0-73c4879b860d"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Q5: UMAP — parameter grid search\n",
    "# ============================================================\n",
    "import umap\n",
    "\n",
    "n_neighbors_list = [5, 10, 15]\n",
    "min_dist_list    = [0.1, 0.5, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 13))\n",
    "fig.suptitle(\"UMAP — n_neighbors vs min_dist (test set)\", fontsize=13)\n",
    "\n",
    "markers = [\"^\", \"s\", \"o\"]\n",
    "\n",
    "for row, nn in enumerate(n_neighbors_list):\n",
    "    for col, md in enumerate(min_dist_list):\n",
    "        ax = axes[row][col]\n",
    "        um = umap.UMAP(n_components=2, random_state=RANDOM_STATE,\n",
    "                       n_neighbors=nn, min_dist=md)\n",
    "        Z_tr = um.fit_transform(Xtr_s, y_train)\n",
    "        Z_te = um.transform(Xte_s)\n",
    "\n",
    "        for c in np.unique(y_test):\n",
    "            ax.scatter(Z_te[y_test == c, 0], Z_te[y_test == c, 1],\n",
    "                       marker=markers[int(c)], s=35, label=f\"class {int(c)+1}\")\n",
    "        ax.set_title(f\"n_neighbors={nn}, min_dist={md}\")\n",
    "        ax.set_xlabel(\"UMAP-1\")\n",
    "        ax.set_ylabel(\"UMAP-2\")\n",
    "        if row == 0 and col == 0:\n",
    "            ax.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}